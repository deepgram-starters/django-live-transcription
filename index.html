<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Django Live Transcription</title>
    <link rel="stylesheet" href="/static/deepgram-styles.css">
</head>
<body>
    <div class="dg-card dg-constrain-width dg-spacing-mobile-compact">
        <h1 class="dg-hero-title">Django Live Transcription</h1>

        <div class="dg-action-group">
            <button id="startBtn" class="btn btn--primary">Start Transcription</button>
            <button id="stopBtn" class="btn btn--danger-ghost" disabled>Stop Transcription</button>
            <button id="restartBtn" class="btn btn--secondary" disabled>Restart Connection</button>
        </div>

        <div id="status" class="dg-status dg-status--error">Disconnected</div>

        <div id="transcription" class="dg-code-block">
            <pre><code>Click "Start Transcription" and speak into your microphone to see live transcription results here.</code></pre>
        </div>
    </div>

    <script>
        let ws = null;
        let isRecording = false;

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const restartBtn = document.getElementById('restartBtn');
        const status = document.getElementById('status');
        const transcription = document.getElementById('transcription');

        function updateStatus(message, isConnected = false) {
            status.textContent = message;
            status.className = `dg-status ${isConnected ? 'dg-status--success' : 'dg-status--error'}`;
        }



        function clearTranscription() {
            const codeElement = transcription.querySelector('code');
            codeElement.innerHTML = 'Ready to transcribe. Speak into your microphone...';
        }

        function showError(message) {
            const codeElement = transcription.querySelector('code');
            const errorSpan = document.createElement('span');
            errorSpan.style.color = 'var(--dg-danger)';
            errorSpan.textContent = 'âŒ ' + message + '\n';
            codeElement.appendChild(errorSpan);
        }

          let microphone = null;

        async function getMicrophone() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                return new MediaRecorder(stream, { mimeType: "audio/webm" });
            } catch (error) {
                console.error("Error accessing microphone:", error);
                throw error;
            }
        }

        async function openMicrophone(microphone, ws) {
            return new Promise((resolve) => {
                microphone.onstart = () => {
                    console.log("ðŸŽ¤ Microphone opened");
                    resolve();
                };

                microphone.ondataavailable = async (event) => {
                    console.log("ðŸ“¡ Audio data:", event.data.size, "bytes");
                    if (event.data.size > 0 && ws && ws.readyState === WebSocket.OPEN) {
                        ws.send(event.data);
                    }
                };

                microphone.start(1000); // Send chunks every 1000ms (like Flask)
            });
        }

                async function startTranscription() {
                                    try {
                console.log('ðŸŽ¤ Starting transcription with MediaRecorder (WebM)...');

                microphone = await getMicrophone();
                console.log("âœ… Microphone access granted");

                // Setup WebSocket
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                ws = new WebSocket(`${protocol}//${window.location.host}/ws/transcription/`);

                ws.onopen = () => {
                    updateStatus('Connected - Starting transcription...', true);
                    clearTranscription();

                    // Send toggle transcription message
                    ws.send(JSON.stringify({type: 'toggle_transcription'}));

                    startBtn.disabled = true;
                    stopBtn.disabled = false;
                    restartBtn.disabled = false;
                };

                                ws.onmessage = (event) => {
                    const data = JSON.parse(event.data);
                    console.log('Received:', data.type);

                    if (data.type === 'transcription_update') {
                        const codeElement = transcription.querySelector('code');
                        codeElement.textContent = data.transcription;
                        transcription.scrollTop = transcription.scrollHeight;
                    } else if (data.type === 'transcription_status') {
                        updateStatus(`Connected - ${data.status}`, true);
                    } else if (data.type === 'error') {
                        showError(data.message);
                    }
                };

                ws.onclose = () => {
                    updateStatus('Disconnected');
                    stopRecording();
                };

                ws.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    updateStatus('Connection error');
                    showError('WebSocket connection failed');
                };

                                // Start microphone recording
                console.log("ðŸŽ™ï¸ Waiting to open microphone");
                await openMicrophone(microphone, ws);
                isRecording = true;

            } catch (error) {
                console.error('Error starting transcription:', error);
                showError('Failed to access microphone: ' + error.message);
                updateStatus('Microphone access denied');
            }
        }

                                        function stopRecording() {
            if (isRecording && microphone) {
                console.log("ðŸ”‡ Stopping microphone...");
                microphone.stop();
                microphone.stream.getTracks().forEach(track => track.stop());
                microphone = null;
                isRecording = false;
            }

            startBtn.disabled = false;
            stopBtn.disabled = true;
            restartBtn.disabled = true;
        }

        function stopTranscription() {
            if (ws) {
                ws.send(JSON.stringify({type: 'toggle_transcription'}));
                ws.close();
                ws = null;
            }
            stopRecording();
            updateStatus('Stopped');
        }

        function restartTranscription() {
            if (ws) {
                ws.send(JSON.stringify({type: 'restart_deepgram'}));
            }
        }

        startBtn.addEventListener('click', startTranscription);
        stopBtn.addEventListener('click', stopTranscription);
        restartBtn.addEventListener('click', restartTranscription);

        // Initialize
        updateStatus('Ready to start');
    </script>
</body>
</html>
